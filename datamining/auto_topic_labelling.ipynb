{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from khaiii import KhaiiiApi\n",
    "import gensim\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   no  year                     title                      kor_full\n",
       "0   1  1997         멀티미디어 원격교육에 관한 연구         멀티미디어 원격교육에 관한 연구    \n",
       "1   2  1997  교육용 하이퍼미디어 자료 편집기에 관한 연구  교육용 하이퍼미디어 자료 편집기에 관한 연구    \n",
       "2   3  1997     인터넷 기반의 코스웨어의 설계 및 구현     인터넷 기반의 코스웨어의 설계 및 구현    \n",
       "3   4  1997     Web에서의 협력 환경 구축 방안 연구     Web에서의 협력 환경 구축 방안 연구    \n",
       "4   5  1997        열린교육에서의 개별화수업과 CAI        열린교육에서의 개별화수업과 CAI    "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>no</th>\n      <th>year</th>\n      <th>title</th>\n      <th>kor_full</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1997</td>\n      <td>멀티미디어 원격교육에 관한 연구</td>\n      <td>멀티미디어 원격교육에 관한 연구</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1997</td>\n      <td>교육용 하이퍼미디어 자료 편집기에 관한 연구</td>\n      <td>교육용 하이퍼미디어 자료 편집기에 관한 연구</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1997</td>\n      <td>인터넷 기반의 코스웨어의 설계 및 구현</td>\n      <td>인터넷 기반의 코스웨어의 설계 및 구현</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1997</td>\n      <td>Web에서의 협력 환경 구축 방안 연구</td>\n      <td>Web에서의 협력 환경 구축 방안 연구</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1997</td>\n      <td>열린교육에서의 개별화수업과 CAI</td>\n      <td>열린교육에서의 개별화수업과 CAI</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data = pd.read_csv(\"./modi_data/kor_full.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWordList = pd.read_excel(\"./khaiii_word_cor.xlsx\")\n",
    "dfWordList2 = pd.read_excel(\"./khaiii_word_cor_etc.xlsx\")\n",
    "#print(dfWordList2)\n",
    "\n",
    "dfWordDel = dfWordList[dfWordList[\"수정\"] == \"삭제\"]\n",
    "dfWordMod = dfWordList[dfWordList[\"수정\"] != \"삭제\"]\n",
    "dfWordDiv = dfWordList2\n",
    "#print(dfWordMod)\n",
    "\n",
    "seriesDelete = dfWordDel[\"기존\"]\n",
    "stopword = []\n",
    "for word in seriesDelete.values:\n",
    "    stopword.append(word)\n",
    "#print(stopword)\n",
    "\n",
    "seriesModify = dfWordMod[\"기존\"]\n",
    "modiword = []\n",
    "for word in seriesModify.values:\n",
    "    modiword.append(word)\n",
    "#print(len(modiword))\n",
    "\n",
    "seriesModify2 = dfWordMod[\"수정\"]\n",
    "modiword2 = []\n",
    "for word in seriesModify2.values:\n",
    "    modiword2.append(word)\n",
    "#print(len(modiword2))\n",
    "\n",
    "seriesDivide = dfWordDiv[\"기존\"]\n",
    "divword = []\n",
    "for word in seriesDivide.values:\n",
    "    divword.append(word)\n",
    "#print(divword)\n",
    "#print(len(divword))\n",
    "\n",
    "seriesDivide2 = dfWordDiv[\"수정\"]\n",
    "divword2 = []\n",
    "for words in seriesDivide2.values:\n",
    "    divword2.append(words.split(', '))\n",
    "#print(divword2)\n",
    "#print(len(divword2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KhaiiiApi()\n",
    "def khaiiiTokenizer(raw, stopword=stopword, pos=['NNG', 'NNP', 'NNB', 'NP', 'NR', 'SL']): # 일반명사 고유명사 의존명사 대명사 수사 외국어\n",
    "    list = []\n",
    "    skip = 0\n",
    "\n",
    "    for word in api.analyze(raw): #raw data\n",
    "        #print(word)\n",
    "        \n",
    "        for i, morph in enumerate(word.morphs):\n",
    "            #print(morph.lex)\n",
    "            if skip == 1: \n",
    "                #print(morph.lex) # '지능'\n",
    "                skip = 0\n",
    "                continue\n",
    "\n",
    "            if morph.lex == '인공' and i+1 < len(word.morphs) and word.morphs[i+1].lex == \"지능\":\n",
    "                #print(morph.lex + word.morphs[i+1].lex) # 인공지능\n",
    "                list.append(morph.lex + word.morphs[i+1].lex)\n",
    "                skip = 1\n",
    "                continue\n",
    "\n",
    "            if len(morph.lex) > 1 and morph.tag in pos and morph.lex not in stopword: \n",
    "                if morph.tag == 'SL':\n",
    "                    morph.lex = morph.lex.lower()\n",
    "                if morph.lex in divword:\n",
    "                    morph.lex = divword2[divword.index(morph.lex)]\n",
    "                    list.extend(morph.lex)\n",
    "                elif morph.lex in modiword:\n",
    "                    morph.lex = modiword2[modiword.index(morph.lex)]\n",
    "                    list.append(morph.lex)\n",
    "                else: list.append(morph.lex)\n",
    "                \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0                                     [멀티미디어, 원격, 교육, 연구]\n1                               [교육, 하이퍼미디어, 자료, 편집기, 연구]\n2                                     [인터넷, 코스웨어, 설계, 구현]\n3                               [web, 협력, 환경, 구축, 방안, 연구]\n4                                       [교육, 개별, 수업, cai]\n                              ...                        \n1144    [개정, 교과서, 소프트웨어, 교육, 단원, 탐구, 비교, 분석, 교육, 과정, 교...\n1145    [이러닝, 콘텐츠, 사용자, 경험, ux, 평가, 이러닝, 대리, 상호, 작용, 사...\n1146    [초등, 데이터, 리터러시, 함양, ai, 데이터, 과학, 교육, 프로그램, 개발,...\n1147    [초등, 예비, 교사, 소프트웨어, 교육, 온라인, 교육, 효과, 분석, 소프트웨어...\n1148    [초등, 교과서, 소프트웨어교육, 영역, 컴퓨팅, 사고력, 요소, 분석, 소프트웨어...\nName: kor_full, Length: 1149, dtype: object\n========= tokenization completed =========\n"
     ]
    }
   ],
   "source": [
    "tokenized = data[\"kor_full\"].apply(lambda row: khaiiiTokenizer(row))\n",
    "print(tokenized)\n",
    "#tokenized.to_csv(\"./modi_data/token_khaiii.csv\")\n",
    "print(\"========= tokenization completed =========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# words in total :  5086\n# documents :  1149\n"
     ]
    }
   ],
   "source": [
    "#lda\n",
    "id2word = gensim.corpora.Dictionary(tokenized)\n",
    "\n",
    "corpus=[id2word.doc2bow(text) for text in tokenized]\n",
    "#print(\"id2word for each document : \", corpus)\n",
    "print(\"# words in total : \", len(id2word))\n",
    "print(\"# documents : \", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0,\n  '0.072*\"수업\" + 0.052*\"활용\" + 0.050*\"문제\" + 0.041*\"학생\" + 0.041*\"대상\" + 0.040*\"효과\" '\n  '+ 0.036*\"적용\" + 0.029*\"연구\" + 0.027*\"모형\" + 0.026*\"중심\"'),\n (1,\n  '0.120*\"인공지능\" + 0.094*\"ai\" + 0.078*\"데이터\" + 0.078*\"창의\" + 0.071*\"알고리즘\" + '\n  '0.040*\"융합\" + 0.037*\"프로그램\" + 0.028*\"모델\" + 0.026*\"탐색\" + 0.020*\"분야\"'),\n (2,\n  '0.138*\"측정\" + 0.072*\"정도\" + 0.067*\"it\" + 0.036*\"문서\" + 0.030*\"타당도\" + '\n  '0.030*\"산출\" + 0.021*\"범주\" + 0.015*\"xml\" + 0.013*\"상관관계\" + 0.011*\"밀접\"'),\n (3,\n  '0.318*\"평가\" + 0.078*\"유형\" + 0.060*\"수행\" + 0.042*\"플랫폼\" + 0.033*\"단원\" + '\n  '0.029*\"이러닝\" + 0.027*\"절차\" + 0.021*\"발표\" + 0.021*\"관찰\" + 0.017*\"iptv\"'),\n (4,\n  '0.117*\"로봇\" + 0.081*\"영향\" + 0.047*\"분석\" + 0.040*\"교과서\" + 0.033*\"유의미\" + '\n  '0.029*\"빅데이터\" + 0.028*\"긍정\" + 0.027*\"사용\" + 0.026*\"연수\" + 0.025*\"통계\"'),\n (5,\n  '0.106*\"컴퓨팅\" + 0.105*\"프로그래밍\" + 0.087*\"사고력\" + 0.046*\"프로그램\" + 0.043*\"교육\" + '\n  '0.039*\"초등\" + 0.024*\"사고\" + 0.023*\"게임\" + 0.022*\"소프트웨어교육\" + 0.021*\"개발\"'),\n (6,\n  '0.060*\"시스템\" + 0.046*\"온라인\" + 0.041*\"집단\" + 0.032*\"확인\" + 0.029*\"기준\" + '\n  '0.028*\"이용\" + 0.025*\"전문가\" + 0.024*\"학생\" + 0.020*\"산업\" + 0.018*\"지원\"'),\n (7,\n  '0.065*\"디지털\" + 0.062*\"스크래치\" + 0.059*\"환경\" + 0.032*\"상호\" + 0.027*\"작용\" + '\n  '0.022*\"자동\" + 0.021*\"구체\" + 0.021*\"정책\" + 0.020*\"설계\" + 0.019*\"사용자\"'),\n (8,\n  '0.173*\"검사\" + 0.060*\"지능\" + 0.048*\"문항\" + 0.043*\"진로\" + 0.039*\"선행\" + 0.037*\"진단\" '\n  '+ 0.033*\"보조\" + 0.024*\"직업\" + 0.023*\"오류\" + 0.020*\"다중\"'),\n (9,\n  '0.169*\"학습\" + 0.043*\"학습자\" + 0.039*\"교수\" + 0.034*\"활동\" + 0.020*\"연구\" + '\n  '0.018*\"요소\" + 0.017*\"단계\" + 0.016*\"개발\" + 0.014*\"이해\" + 0.014*\"방법\"'),\n (10,\n  '0.156*\"교육\" + 0.040*\"연구\" + 0.037*\"분석\" + 0.035*\"초등\" + 0.032*\"정보\" + 0.028*\"과정\" '\n  '+ 0.028*\"sw\" + 0.027*\"교사\" + 0.020*\"컴퓨터\" + 0.018*\"소프트웨어\"'),\n (11,\n  '0.089*\"개정\" + 0.061*\"지속\" + 0.049*\"원격\" + 0.048*\"국내\" + 0.047*\"발문\" + 0.036*\"대응\" '\n  '+ 0.032*\"동향\" + 0.027*\"우수\" + 0.026*\"어플리케이션\" + 0.025*\"저작\"'),\n (12,\n  '0.122*\"ict\" + 0.076*\"활용\" + 0.074*\"능력\" + 0.061*\"리터러시\" + 0.047*\"스마트폰\" + '\n  '0.031*\"강의\" + 0.028*\"학기\" + 0.022*\"학생\" + 0.020*\"sns\" + 0.019*\"수준\"')]\n"
     ]
    }
   ],
   "source": [
    "a = int(input(\"set the number of topics \"))  # 13 or 11\n",
    "optimal_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=a,\n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                iterations=1000,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                eta='auto',\n",
    "                                                per_word_topics=True)\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "pprint(optimal_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get p(word|topic)\n",
    "def get_topic_word_prob(lda_model):\n",
    "    topic_word_freq = lda_model.state.get_lambda()\n",
    "    topic_word_prob = topic_word_freq / topic_word_freq.sum(axis=1)[:, None]\n",
    "\n",
    "    return topic_word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(13, 5086)\n          교육     멀티미디어        연구        원격        자료       편집기    하이퍼미디어  \\\n0   0.002215  0.000195  0.028537  0.000005  0.000011  0.000005  0.000005   \n1   0.001012  0.000014  0.000014  0.000014  0.000014  0.000014  0.000014   \n2   0.000099  0.000099  0.000099  0.000099  0.000099  0.000099  0.000099   \n3   0.000032  0.000032  0.000032  0.000032  0.000032  0.000032  0.000032   \n4   0.000014  0.000014  0.012572  0.000014  0.000014  0.000014  0.000014   \n5   0.043205  0.000007  0.011587  0.000007  0.000007  0.000097  0.000007   \n6   0.000011  0.000011  0.002519  0.000011  0.003488  0.000011  0.000011   \n7   0.000018  0.004570  0.001044  0.000018  0.000018  0.000018  0.000612   \n8   0.000051  0.000051  0.000051  0.000051  0.000051  0.000051  0.000051   \n9   0.000005  0.000005  0.020482  0.000005  0.009867  0.000005  0.000005   \n10  0.156253  0.000002  0.039828  0.000002  0.005235  0.000002  0.000002   \n11  0.000058  0.000058  0.000058  0.049458  0.000058  0.000058  0.000058   \n12  0.000033  0.000176  0.000033  0.000033  0.000033  0.000033  0.000033   \n\n          구현        설계       인터넷  ...       new    normal       뉴노멀        대리  \\\n0   0.000005  0.006691  0.000005  ...  0.000005  0.000005  0.000005  0.000005   \n1   0.000014  0.000014  0.000014  ...  0.000014  0.000014  0.000014  0.000014   \n2   0.000099  0.000099  0.000099  ...  0.000099  0.000099  0.000099  0.000099   \n3   0.000032  0.000032  0.000032  ...  0.000034  0.000034  0.000034  0.000034   \n4   0.000014  0.000014  0.015215  ...  0.000014  0.000014  0.000014  0.000014   \n5   0.000007  0.000008  0.000007  ...  0.000007  0.000007  0.000007  0.000007   \n6   0.007855  0.014634  0.009269  ...  0.000011  0.000011  0.000011  0.000011   \n7   0.018174  0.019684  0.000018  ...  0.000019  0.000019  0.000019  0.000019   \n8   0.000051  0.000051  0.000051  ...  0.000051  0.000051  0.000051  0.000051   \n9   0.003644  0.011095  0.000005  ...  0.000005  0.000005  0.000005  0.000005   \n10  0.000002  0.000002  0.000002  ...  0.000002  0.000002  0.000002  0.000002   \n11  0.000058  0.000058  0.000058  ...  0.000058  0.000058  0.000058  0.000058   \n12  0.000033  0.000033  0.000033  ...  0.000033  0.000033  0.000033  0.000033   \n\n          독백       파급력      바이러스      교과서별       발견과       출판사  \n0   0.000005  0.000005  0.000005  0.000005  0.000005  0.000005  \n1   0.000015  0.000015  0.000014  0.000014  0.000014  0.000014  \n2   0.000099  0.000099  0.000099  0.000099  0.000099  0.000099  \n3   0.000039  0.000032  0.000032  0.000032  0.000032  0.000032  \n4   0.000014  0.000014  0.000014  0.000014  0.000014  0.000014  \n5   0.000008  0.000008  0.000008  0.000008  0.000008  0.000008  \n6   0.000011  0.000011  0.000011  0.000011  0.000011  0.000011  \n7   0.000020  0.000018  0.000018  0.000018  0.000018  0.000018  \n8   0.000051  0.000052  0.000051  0.000051  0.000051  0.000051  \n9   0.000005  0.000005  0.000005  0.000005  0.000005  0.000005  \n10  0.000002  0.000002  0.000002  0.000002  0.000002  0.000002  \n11  0.000058  0.000059  0.000058  0.000058  0.000058  0.000058  \n12  0.000033  0.000034  0.000034  0.000033  0.000033  0.000033  \n\n[13 rows x 5086 columns]\n"
     ]
    }
   ],
   "source": [
    "topic_word_prob = get_topic_word_prob(optimal_model)\n",
    "print(topic_word_prob.shape) # (#topics, #words)\n",
    "\n",
    "wordlist = []\n",
    "for i in range(len(id2word)):\n",
    "    #print(id2word[i])\n",
    "    wordlist.append(id2word[i])\n",
    "#print(wordlist)\n",
    "seriesWordlist = pd.Series(wordlist)\n",
    "\n",
    "topic_word = pd.DataFrame(data=topic_word_prob[0:, 0:], columns=seriesWordlist)\n",
    "\n",
    "print(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-gram candidates\n",
    "\n",
    "def get_ngrams(raw, n_range=(1,3)): # 1~3-gram 까지\n",
    "\n",
    "    def to_ngrams(words, n):\n",
    "        ngrams = []\n",
    "        for b in range(0, len(words) - n + 1):\n",
    "            ngrams.append(str(tuple(words[b:b+n])))\n",
    "        return ngrams\n",
    "\n",
    "    n_begin, n_end = n_range\n",
    "    ngram_list = []\n",
    "    \n",
    "    for n in range(n_begin, n_end + 1):\n",
    "        for ngram in to_ngrams(raw, n):\n",
    "            ngram_list.append(ngram)\n",
    "            \n",
    "    return ngram_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       [('멀티미디어',), ('원격',), ('교육',), ('연구',), ('멀티미디...\n1       [('교육',), ('하이퍼미디어',), ('자료',), ('편집기',), ('연구...\n2       [('인터넷',), ('코스웨어',), ('설계',), ('구현',), ('인터넷'...\n3       [('web',), ('협력',), ('환경',), ('구축',), ('방안',),...\n4       [('교육',), ('개별',), ('수업',), ('cai',), ('교육', '...\n                              ...                        \n1144    [('개정',), ('교과서',), ('소프트웨어',), ('교육',), ('단원'...\n1145    [('이러닝',), ('콘텐츠',), ('사용자',), ('경험',), ('ux',...\n1146    [('초등',), ('데이터',), ('리터러시',), ('함양',), ('ai',...\n1147    [('초등',), ('예비',), ('교사',), ('소프트웨어',), ('교육',...\n1148    [('초등',), ('교과서',), ('소프트웨어교육',), ('영역',), ('컴...\nName: kor_full, Length: 1149, dtype: object\n<class 'pandas.core.series.Series'>\n<class 'list'>\n<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "ngrams = tokenized.apply(lambda row: get_ngrams(row))\n",
    "\n",
    "print(ngrams)\n",
    "print(type(ngrams))\n",
    "print(type(ngrams.loc[0]))\n",
    "print(type(ngrams.loc[0][0]))\n",
    "#ngrams.to_csv('./modi_data/ngrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score(l, t)\n",
    "# score = sum_w(p(w|t) * PMI(w,l|c))\n",
    "\n",
    "def get_score(unigram_counter, ngram_candidates, candidate, topic, topic_word_prob, ngram_docs):\n",
    "    def get_pmi(w, l_freq, unigram_counter, docs):\n",
    "        word = \"('\" + w +\"',)\"\n",
    "        count = 0\n",
    "\n",
    "        for doc in docs:\n",
    "            #print(doc)\n",
    "            if word in doc: \n",
    "                count += 1\n",
    "        pmi = count / (unigram_counter.loc[0, w] * l_freq)\n",
    "        return pmi\n",
    "    \n",
    "    candi_docs = []\n",
    "    print(type(candi_docs))\n",
    "    for doc in ngram_docs:\n",
    "        if candidate in doc:\n",
    "            candi_docs.append(doc)\n",
    "    #print(candi_docs)\n",
    "\n",
    "    for i in range(len(ngram_candidates)):\n",
    "        if ngram_candidates.loc[i, '후보명'] == candidate:\n",
    "            candi_count = ngram_candidates.loc[i, 'count']\n",
    "    print(candidate, candi_count)\n",
    "\n",
    "    score = 0\n",
    "    for word in unigram_counter.columns:\n",
    "        tw_prob = topic_word_prob.loc[topic, word]\n",
    "        pmi = get_pmi(word, candi_count, unigram_counter, candi_docs)\n",
    "        score += tw_prob * pmi\n",
    "\n",
    "    return round(score * 1000, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n",
      "('통신', '윤리', '교육') 59\n",
      "0.16266\n"
     ]
    }
   ],
   "source": [
    "nc = pd.read_excel(\"./modi_data/ngram_candidates.xlsx\")\n",
    "uc = pd.read_excel(\"./modi_data/unigram_counter.xlsx\")\n",
    "nc = nc.drop(['Unnamed: 0'], axis=1)\n",
    "uc = uc.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "#print(nc.columns[0])\n",
    "#print(nc.loc[0, '후보명'])\n",
    "#print(topic_word.loc[0, '교육'])\n",
    "#score1 = get_score(uc, nc, nc.loc[21, '후보명'], 0, topic_word, ngrams)\n",
    "\n",
    "#print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic 0\n",
    "for i in range(len(nc)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score(l, t)\n",
    "# score = sum_w(p(w|t) * PMI(w,l|c))\n",
    "\"\"\"\n",
    "def get_score(unigram_counter, ngram_candidates, candidate, topic, topic_word_prob, ngram_docs):\n",
    "    def get_pmi(w, l, l_freq, unigram_counter, docs):\n",
    "        word = \"('\" + w +\"',)\"\n",
    "        #print(word)\n",
    "        count = 0\n",
    "\n",
    "        for doc in docs:\n",
    "            #print(doc)\n",
    "            if word in doc and l in doc:#and word in doc:\n",
    "                #print('yesss')\n",
    "                count += 1\n",
    "\n",
    "        pmi = count / (unigram_counter.loc[0, w] * l_freq)\n",
    "        return pmi\n",
    "    \n",
    "    for i in range(len(ngram_candidates)):\n",
    "        if ngram_candidates.loc[i, '후보명'] == candidate:\n",
    "            candi_count = ngram_candidates.loc[i, 'count']\n",
    "    print(candidate, candi_count)\n",
    "\n",
    "    score = 0\n",
    "    for word in unigram_counter.columns:\n",
    "        tw_prob = topic_word_prob.loc[topic, word]\n",
    "        pmi = get_pmi(word, candidate, candi_count, unigram_counter, ngram_docs)\n",
    "        score += tw_prob * pmi\n",
    "\n",
    "    return score\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "ModuleNotFoundError",
     "traceback": [
      "Error: ModuleNotFoundError",
      "at b.parseConnectInfo (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:49:486235)",
      "at b.connectToLocal (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:49:486838)",
      "at async b.connect (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:49:484715)",
      "at async b.startDebugSession (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:49:483862)",
      "at async k.submitCode (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:32:585464)",
      "at async k.handleRunByLine (/Users/uni613/.vscode/extensions/ms-toolsai.jupyter-2020.12.414227025/out/client/extension.js:9:173871)"
     ]
    }
   ],
   "source": [
    "# get PMI(word, 후보명(l)|문맥(c))\n",
    "# PMI = (#(w,l)) / (#w * #l)\n",
    "\"\"\"\n",
    "def get_pmi(unigram_counter, ngram_candidates, ngram_docs):\n",
    "    dfPMI = pd.DataFrame()\n",
    "    pmi_list = []\n",
    "\n",
    "    def get_wl_count(word, candidate, docs):\n",
    "        word = '(' + word + ',)'\n",
    "        count = 0\n",
    "\n",
    "        for doc in docs:\n",
    "            if candidate in doc and word in doc:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    for i, l in enumerate(ngram_candidates['후보명']):\n",
    "        for w in unigram_counter.columns:\n",
    "            pmi = get_wl_count(w, l, ngram_docs) / (unigram_counter.loc[0, w] * ngram_candidates.loc[i, 'count'])\n",
    "            pmi_list.append(pmi) \n",
    "            print('word')   \n",
    "        dfPMI.append(pmi_list)\n",
    "        pmi_list = []\n",
    "        print('5000')\n",
    "\n",
    "    dfPMI.columns = unigram_counter.columns\n",
    "\n",
    "    return dfPMI\n",
    "    \"\"\""
   ]
  }
 ]
}